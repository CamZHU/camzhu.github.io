---
layout: post
comments: false
title:  "Chapter 4 of Adam White's Thesis [1]"
excerpt: "Cumulant, reinforcement learning, General Value Function"
date:   2016-09-09
mathjax: true
---


[*Developing a predictive approach to knowledge*](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.698.7096&rep=rep1&type=pdf)

It is [said](http://www.nature.com/news/the-past-present-and-future-of-the-phd-thesis-1.20207) that, on average, 1.6 people will read your PhD thesis.
I don't think PhD thesis is that bad, so I decided to start reading other PhD's thesis occasionally.

Adam White is a recent graduate from University of Alberta's RL lab.
I feel strongly obligated to learn the most advance knowledge about RL from his thesis. 
One thing that thesis is particularly reading material is the systematic presentation of the cutting-edge knowledge.
For sure, we can find review papers from top journal that surveys relevant literatures for readers.
However, thesis is structured better and expand to larger scope than a review paper, say, in *Tren in Cog Sci*.


I have recently read Adam White's curiosity and surprise paper.
The paper mentioned and utilized General Value Function, which is also the theme for Chapter 4.
It is a new perspective of looking at the value function.
I believe that there are many follow-up papers and one of them got into top ML conference is the [Schaul et al (2015) ICML](http://jmlr.org/proceedings/papers/v37/schaul15.pdf). 

The idea is simplely leveraging the strengths of value function in predictive knowledge.
As it is generlized version of the conventional value function, it must be placed into a broader setting.

(1) The target of prediction: reward --> answer to some specified questions

(2) Computation of the target: magnitude of reward + discount factor --> cumulant + termination signal

So now, reward signal is merely one kind of the cumulant signals.
An agent can definitely learn multiple cumulant signals in parallel.

You may naturally think of an agent must be a cumulant-maximizer then.
Not necessarily!

The behavioral policy can be reward maximizing as before.
You can imagine an agent pick some of the cumulant to assist decision making.
It is the usefulness of some kind of cumulant for achieving current goal that determines how would a cumulant being used to control.

This is an interesting area to explore.
In Adam's example, 

> Consider an agent driving a car from one location to another. [...] We may specify several cumulants, such as a cumulant equal to the continuous fuel consumption of the car, or a cumulant equal to one when the car is too close to the car in front of it. A third cumulant could equal to the temperature outside the car. This third cumulant is not obviously related to the goal of driving from one location to another quickly, but predicting future temperature may be useful for other tasks, or might be best thought of as general knowledge of the agentâ€™s environment.

I can see some irrational behaviors that were well-documented in behavioral economcis literature can be looked through this lense. 







