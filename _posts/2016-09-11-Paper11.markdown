---
layout: post
comments: false
title:  "Policy Gradient [1]"
excerpt: "reinforcement learning"
date:   2016-09-11
mathjax: true
---

I recently watched [Pieter Abbeel's deep RL lecture on DLSS 2016](http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/). 
The lecture mainly introduced the policy gradient method, which is now the new standard in Deep RL.

My intuition for the policy gradient method is that his approach try to reduce reinforcement learning problem to a superviser learning or a optimization problem.

What is the mapping?
Policy network take the state as input and generate (probability distribution of) actions directly.
Intuitively, if an action sequence incur higher rewards, then we should increase the probability of such sequence through adjusting parameters in the policy network $$ ^{[1]}$$.

Here is how these ideas presented in formal models (the objective function):

\begin{equation}
\max_\theta U(\theta)=\max_\theta \sum_\tau P(\tau ; \theta)R(\tau)
\end{equation}

where $$\tau$$ is one trajectory following actions generated by the current parameterization in the policy network.

Now we can utilize the $$\nabla_\theta U(\theta)$$ as teaching signals to adjust parameters in order to realize the idea $$ ^{[1]}$$.
After some math, we are able to obtain the `Likelihood Ratio Policy Gradient`:

\begin{align}
\nabla_\theta U(\theta) & =\sum_\tau P(\tau ; \theta) \nabla_\theta logP(\tau ; \theta) R(\tau) \\
E_{m~P(\tau ; \theta)}[\nabla_\theta U(\theta)] & = E_{m~P(\tau ; \theta)}[\nabla_\theta logP(\tau ; \theta) R(\tau)] \\
& = \frac{1}{m} \sum_{i=1]^{m} \nabla_\theta logP(\tau ; \theta) R(\tau)
\end{align}









