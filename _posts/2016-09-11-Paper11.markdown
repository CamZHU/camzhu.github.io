---
layout: post
comments: false
title:  "Policy Gradient [1]"
excerpt: "reinforcement learning, deep learning"
date:   2016-09-11
mathjax: true
---

I recently watched [Pieter Abbeel's deep RL lecture on DLSS 2016](http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/). 
The lecture mainly introduced the policy gradient method, which is now the new standard in Deep RL.

My intuition for the policy gradient method is that his approach try to reduce reinforcement learning problem to a superviser learning or a optimization problem.

What is the mapping?
Policy network take the state as input and generate (probability distribution of) actions directly.
Intuitively, if an action sequence incur higher rewards, then we should increase the probability of such sequence through adjusting parameters in the policy network $$ ^{[1]}$$.

Here is how these ideas presented in formal models (the objective function):

\begin{equation}
\max_\theta U(\theta)=\max_\theta \sum_\tau P(\tau ; \theta)R(\tau)
\end{equation}

where $$\tau$$ is one trajectory following actions generated by the current parameterization in the policy network.

Now we can utilize the $$\nabla_\theta U(\theta)$$ as teaching signals to adjust parameters in order to realize the idea $$ ^{[1]}$$.
After some math, we are able to obtain the `Likelihood Ratio Policy Gradient`:

\begin{equation}
\nabla_\theta U(\theta) =\sum_\tau P(\tau ; \theta) \nabla_\theta logP(\tau ; \theta) R(\tau)
\end{equation}


\begin{align}
E_{m \sim P(\tau ; \theta)}[\nabla_\theta U(\theta)] & = E_{m~P(\tau ; \theta)}[\nabla_\theta logP(\tau ; \theta) R(\tau)] \\
& = 1/m \sum_{i=1}^{m}[\nabla_\theta logP(\tau ; \theta) R(\tau)]
\end{align}


Appealing properties magically arise:

(1) As long as $$logP(\tau ; \theta)$$ is differentiable, we are able to get the policy gradient even if reward is discrete

Further breakdown of the $$ \nabla_\theta logP(\tau ; \theta) $$:
\begin{align}
\nabla_\theta logP(\tau ; \theta) = \sum_{t=0}^{H} \nabla_\theta log \pi_\theta(u_t|s_t)
\end{align}

(2) The log likelihood of a trajectory can be calculated without knowing the dynamic model (aka. environment dynamic). Note that, in classical control literature, $$u$$ denotes action.

The above equations are at core of the policy gradient method.
Let's do a bit reflections/evaluations.

This method, as it is mentioned in [Andrej's post](http://karpathy.github.io/2016/05/31/rl/), can have very interesting applications in non-differentiable computation in neural network.
Attention can be designed as a form of policy gradient. 
A series of ideas are formalized in [Williams (1992)](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf), [Minh et al (2014) Recurrent Models of Visual Attention](http://arxiv.org/pdf/1406.6247v1.pdf), and [Schulman et al (2015)](http://arxiv.org/pdf/1506.05254v3.pdf)


The method requires to coupling with good exploration strategies. 
It is mainly because of the nature of the policy gradient method is to search for a good trajectory/roll-out that maximize the expected return.
In large state-space conditions, a good exploration strategy is particular in need.





