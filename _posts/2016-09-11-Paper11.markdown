---
layout: post
comments: false
title:  "Policy Gradient [1]"
excerpt: "reinforcement learning"
date:   2016-09-11
mathjax: true
---

I recently watched [Pieter Abbeel's deep RL lecture on DLSS 2016](http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/)

My intuition for the policy gradient method is that his approach try to reduce reinforcement learning problem to a superviser learning or a optimization problem.

What is the mapping?
Policy network take the state as input and generate (probability distribution of) actions directly.
Intuitively, if an action sequence incur higher rewards, then we should increase the probability of such sequence through adjusting parameters in the policy network.

Here is how these ideas presented in formal models (the objective function):

\begin{equation}
\max_\theta U(\theta)=\max_\theta \sum_\tau P(\tau ; \theta)R(\tau)
\end{equation}

where $$\tau$$ is one trajectory following actions generated by the current parameterization in the policy network.






