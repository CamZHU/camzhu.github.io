---
layout: post
comments: false
title:  "Policy Gradient [2]"
excerpt: "reinforcement learning, deep learning"
date:   2016-09-12
mathjax: true
---

Couple more techniques were introduced to policy gradient method.

(1) Baseline

Recall the `likelihood ratio gradient`:

\begin{equation}
\nabla U(\theta) \approx \frac{1}{m} \sum_{i=1}^{m} \nabla_\theta log P(\tau^{(i)}; \theta) R(\tau^{(i)}) 
\end{equation}

Intuitively, when all $$R>0$$, the network tries to increase probabilites of all paths but relatively heightened the path with higher returns.
Introducing some baseline performance, $$b$$, to all paths would result in negative gradient for some not-so-good paths.

\begin{equation}
\nabla U(\theta) \approx \frac{1}{m} \sum_{i=1}^{m} \nabla_\theta log P(\tau^{(i)}; \theta) R(\tau^{(i)} - b) 
\end{equation}

Our method is still unbiased.
In practice, $$b=V^\pi (s_k)$$ indicates the expected long-term return under current policy.

(2) Step sizing and trust region

Step size need to be considered carefully esp in policy gradient because a terrible policy obtained from oversized learning rate will lead to another terrible policy.

First order approximation from gradient can be good within some `'trust region'`.
This region is defined as $$KL(P(\tau;\theta) || P(\tau; \theta+\delta\theta)) \leq \epsilon$$

(3) Variance reduction by discounting

Although it's nature to think of estimation of Q value as $$Q^{\pi, \gamma} (s,u) = E[r_0 + \gamma r_1, \gamma^2 r_2 + \dots | s_0 = s, u_0 = u]$$, discount factor was induced to balance the temporal structure of rewards.
Look at the equation again.
We are using $$\gamma$$ as a hyperparameter to reduce variance.

(4) Variance reduction by function approximation (=critic)


Similar idea can be found in $$TD(\gamma)$$.
We estimate the Q values as $$Q(s,u) = E[r_0 + \gamma r_1  + \gamma^2 V^\pi(s_2) | s_0 = s, u_0 = u]$$

