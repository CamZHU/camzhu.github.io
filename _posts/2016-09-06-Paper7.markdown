---
layout: post
comments: false
title:  "White, Modayil, & Sutton (2014) AAAI"
excerpt: "curiosity, surprise, exploration"
date:   2016-09-06
mathjax: true
---

[*Surprise and Curiosity for Big Data Robotics*](http://homes.soic.indiana.edu/adamw/surprise.pdf)

Some new ideas in RL were presented in the paper such as `general value function (GVF)` and `Mean Squared Projected Bellman Error (MSPBE)`.

In nutshell, GVF generalizes many core components of the original value function: 
\begin{equation}
v(s) = E_\pi[\sum \gamma^k R_{t+k+1} | S_t = s]
\end{equation}

The discount factor is now interpreted as the likelihood of termination, rewards as cumulants (which is accumulated future prediction errors).

In addition, MSPBE is a teaching signal that allows stochastic gradient descent to operate on. 

With these, the authors are able to introduce a rule-based curious behavior (a bit like improved e-greedy).
The control parameter Z is defined as:
\begin{equation}
Z_t^{(i)} = \frac{\delta^{(i)}}{\sqrt{var[\delta^{(i)}]}}
\end{equation}

And the simulation suggests the above model has the lowest BIC score (aka best fit).

The feeling function was further used into predicting gamble choice: `P(gamble)`.
The feeling model outperforms other decision making models in BIC scores.

<img src="/images/paper6_2" style="width: 80%; height: 80%; margin-left: auto; margin-right: auto;">


I suspect fitting feeling functions to individuals might be more intersting because there are intuitively individual differences in emotional responses to same risky outcomes.


