---
layout: post
comments: false
title:  "Some practices in Kaggle Competition"
excerpt: "Kaggle is a fun way to practice machine learning skills, hence I participated in some Kaggle projects"
date:   2015-09-10 17:30:00
mathjax: true
---



I made my debut on Kaggle in 16 Sept, 2015.
The competition I have participated in was [the classical handwriting recognition task](https://www.kaggle.com/c/digit-recognizer).
The rule is simple to make the most accurate out-of-sample prediction of [MNIST data](http://yann.lecun.com/exdb/mnist/).
My result is just OK and you can find it as follows:

<img src="https://raw.githubusercontent.com/CamZHU/camzhu.github.io/master/images/kaggle_digit_result.png" height="500" width="750"/>

My code is simple, which is basically a 3-layer neural network. 
Specifically, the layers from input to output contains rectifier, sigmoid, and softmax neurons in order.
It only took me few minutes to tune with tremendous help from [Keras](http://keras.io/models/).
I did not go for sophisicated deep learning models such as RNN or CNN because I prioritize to quickly adapt to Kaggle rather than climbing ranks. 


Note that the best models are probably around 99% according to [the leaderboard](https://www.kaggle.com/c/digit-recognizer/leaderboard). However, 
I am quite skeptical about the models that were able to achieve this insane accuracy.
Plus the test set is static and leaving lots of chances to cheat.
Above all, the competition gives me strong sense of how fierce it is a 1% improvement in algorithm.
