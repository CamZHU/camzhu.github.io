---
layout: post
comments: false
title:  "Some practices in Kaggle Competition"
excerpt: "I participated in some Kaggle 101 projects recently"
date:   2015-09-10 17:30:00
mathjax: true
---



I made my debut on Kaggle in 16 Sept, 2015.
The competition I have participated in was [the classical handwriting recognition task](https://www.kaggle.com/c/digit-recognizer).
The rule is simple to make the most accurate out-of-sample prediction of [MNIST data](http://yann.lecun.com/exdb/mnist/).
My result is just OK and showing as follows:

<img src="https://raw.githubusercontent.com/CamZHU/camzhu.github.io/master/images/kaggle_digit_result.png" />

My code is simple, which is basically a 3-layer neural network. 
It took me few minutes to tune.
I did not go for sophisicated deep learning models like RNN and CNN because I prioritize to quickly adapt to Kaggle rather than climbing ranks. 
Note that the best models are probably around 99% according to [the leaderboard](https://www.kaggle.com/c/digit-recognizer/leaderboard). However, 
I am skeptical about the models that were able to achieve this insane accuracy.
Plus the test set is static and leaving lots of chances to cheat.
Still, the competition gives me strong sense of how fierce it is a 1% improvement in algorithm.
